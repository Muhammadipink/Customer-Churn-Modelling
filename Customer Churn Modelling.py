# -*- coding: utf-8 -*-
"""Assignment Day 11 - Group 1 (final ver).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rm-KcjcY5DCbuc8PA8VZlg0WEJZhnukD

# Introduction

Anda seorang konsultan data dari salah satu firma konsultan diminta oleh tim bisnis divisi Payment dari sebuah lembaga pinjaman online untuk memprediksi user mana yang tidak akan menggunakan platform mereka lagi untuk melakukan pembayaran online. Mereka sebelumnya pernah meminta konsultan lain untuk membuat modelnya namun konsultan tersebut tidak bisa menjawab mengapa customer diprediksi churn atau tidak berdasarkan model yang dibuat sehingga model tersebut tidak bisa dipakai karena tidak transparan dan tidak bisa memperoleh insight apapun dari model tersebut.

## Metadata

Anda diberikan data dengan beberapa variabel yang diberikan sebagai berikut:
* user_id: customer account number.
* attrition_flag: customer status (Existing and Attrited).
* customer_age: age of the customer.
* gender: gender of customer (M for male and F for female).
* dependent_count: number of dependents of customers.
* education_level: customer education level (Uneducated, High School, Graduate, College, Post-Graduate, Doctorate, and Unknown).
* marital_status: customer's marital status (Single, Married, Divorced, and Unknown).
* income_category: customer income interval category (Less than \$40K, \$40K-\$60K, \$60K-\$80K, \$80K-\$120K, \$120K +, and Unknown).
* card_category: type of card used (Blue, Silver, Gold, and Platinum).
* months_on_book: period of being a customer (in months).
* total_relationship_count: the number of products used by customers in the bank.
* months_inactive_12_mon: period of inactivity for the last 12 months.
* contacts_count_12_mon: the number of interactions between the bank and the customer in the last 12 months.
* credit_limit: credit card transaction nominal limit in one period.
* total_revolving_bal: total funds used in one period.
* avg_open_to_buy: the difference between the credit limit set for the cardholder's account and the current balance.
* total_amt_chng_q4_q1: increase in customer transaction nominal between quarter 4 and quarter 1.
* total_trans_amt: total nominal transaction in the last 12 months.
* total_trans_ct: the number of transactions in the last 12 months.
* total_ct_chng_q4_q1: the number of customer transactions increased between quarter 4 and quarter 1.
* avg_utilization_ratio: percentage of credit card usage.

# Install & Load Library
"""

!pip install dalex
!pip install scikit-plot

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

from xgboost import XGBClassifier

import dalex as dx

import scikitplot as skplt 

# %matplotlib inline

"""# Load Dataset"""

raw_data = pd.read_csv("https://raw.githubusercontent.com/hadimaster65555/dataset_for_teaching/main/dataset/bank_churn_dataset/bank_churn_data.csv")

"""# Data Inspection"""

raw_data

raw_data.info()

"""Check missing values"""

raw_data.isna().sum()

"""Typecasting categorical variables into numeric"""

raw_data['attrition_flag'].replace(['Existing Customer','Attrited Customer'],[0,1],inplace=True)
raw_data['gender'].replace(['M','F'],[0,1],inplace=True)
raw_data['education_level'].replace(['Unknown','Uneducated','High School','College','Graduate','Post-Graduate','Doctorate'],[0,1,2,3,4,5,6],inplace=True)
raw_data['marital_status'].replace(['Unknown','Single','Married','Divorced'],[0,1,2,3],inplace=True)
raw_data['income_category'].replace(['Unknown','Less than $40K','$40K - $60K','$60K - $80K','$80K - $120K','$120K +'],[0,1,2,3,4,5],inplace=True)
raw_data['card_category'].replace(['Blue','Silver','Gold', 'Platinum'],[0,1,2,3],inplace=True)

"""Check new data structure"""

raw_data.info()

"""Change column names"""

raw_data.rename(columns = {'marital_status':'marital status', 'income_category':'income category',
                           'card_category':'card category', 'total_relationship_count':'total product used',
                           'months_inactive_12_mon':'months inactive', 'contacts_count_12_mon': 'total interaction',
                           'total_revolving_bal':'funds used in period', 'total_trans_ct':'total trans freq',
                           'total_ct_chng_q4_q1':'increase trans q4-q1', 'avg_utilization_ratio':'avg cc usage'},
                inplace = True)

# check again the structure
raw_data.info()

"""We need to remove user_id from dataset"""

raw_data = raw_data.drop(["user_id"], axis = 1)

raw_data

"""# Train-Test Split Data

Split data before data exploration and engineering
"""

X = raw_data.drop(["attrition_flag"], axis = 1)
y = raw_data["attrition_flag"]

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y, 
    test_size=0.2,
    stratify = y, 
    random_state=1000
)

"""# Data Exploration"""

X_train["attrition_flag"] = y_train

"""Let's check target distribution"""

sns.catplot(x = "attrition_flag", kind = "count", data = X_train);

"""We can observe there imbalance case in our dataset. We can solve this in two ways:

- Post-modeling, by changing classification threshold to optimize metrics such as F1-Score, precision, recall, etc
- Pre-modeling, by doing resamping such as oversampling, downsampling, and mixed sampling

We gonna do post-modeling (threshold) if necessary

Next we gonna check multicolinearity for each variable

"""

corr = raw_data.corr()
masking = np.triu(np.ones_like(corr, dtype=bool))

f, ax = plt.subplots(figsize=(20, 20))

cmap = sns.diverging_palette(230, 20, as_cmap=True)

sns.heatmap(
    corr, 
    mask=masking, 
    cmap=cmap, 
    vmax=.3, 
    center=0,
    square=True, 
    linewidths=.5,
    annot=True,
    annot_kws={'size':12}
);

"""We observe multiple multicolinearity between avg_open_to_buy vs credit_limit, total_trans_ct vs total_trans_amt, months_on_book vs customer_age, total_revolving_bal vs avg_utilization_ratio. After this, we will analyse those variables more deep in the boxplots.

## Categorical Data vs attrition_flag

**gender vs attrition_flag**
"""

g = sns.FacetGrid(
    data = X_train.groupby(["gender","attrition_flag"], as_index = False).size(),
    col = "attrition_flag",
    sharex = False,
    sharey = False
)
g.map(sns.barplot, "gender", "size");

"""We can observe that female has higher churn rate, we can:
- remove this because gender is one of protected features that need to be removed from dataset
- remove this feature because is has no benefit to improve model accuracy

**education_level vs attrition_flag**
"""

g = sns.FacetGrid(
    data = X_train.groupby(["education_level","attrition_flag"], as_index = False).size(),
    col = "attrition_flag",
    sharex = False,
    sharey = False
)
g.map(sns.barplot, "education_level", "size");

"""It seems that user with graduate education level will more likely to churn. The doctoral, post-graduate, and college customer is lowest three. But we can see the same distribution at the not churn value. We will remove this variable.

**marital_status vs attrition_flag**
"""

g = sns.FacetGrid(
    data = X_train.groupby(["marital status","attrition_flag"], as_index = False).size(),
    col = "attrition_flag",
    sharex = False,
    sharey = False
)
g.map(sns.barplot, "marital status", "size");

"""The user with the marital status of single and married are more likely to churn, while user who divorced is less likely to churn.

**income_category vs attrition_flag**
"""

g = sns.FacetGrid(
    data = X_train.groupby(["income category","attrition_flag"], as_index = False).size(),
    col = "attrition_flag",
    sharex = False,
    sharey = False
)
g.map(sns.barplot, "income category", "size");

"""We can observe that people with lower income (less than $400K) is more likely to churn

**card_category vs attrition_flag**
"""

g = sns.FacetGrid(
    data = X_train.groupby(["card category","attrition_flag"], as_index = False).size(),
    col = "attrition_flag",
    sharex = False,
    sharey = False
)
g.map(sns.barplot, "card category", "size");

"""We can observe that the blue card user will more likely to churn.

## Numerical Data vs attrition_flag

**Age vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "customer_age", data = X_train);

"""We can observe that the distribustion of age both in the churn and not churn is relatively the same. We can drop this variable.

**Dependent count vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "dependent_count", data = X_train);

"""We can observe that the distribustion of dependent count both in the churn and not churn is relatively the same. We can drop this variable.

**Months on Book vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "months_on_book", data = X_train);

"""We can observe that period of being a customer have relatively the same distribution between user who churn and not churn. We can remove this variable.

**Total Relationship Count vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "total product used", data = X_train);

"""We can observe the one with lower relationship count is more likely to churn.

**Months inactive vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "months inactive", data = X_train);

"""We can observe the one with 2-3 months inactivity is more likely to churn. But, it's not too different with user who are not churn, the variance is just bigger.

**Number of Interaction within 12 months vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "total interaction", data = X_train);

"""We can observe that the user with more number of interaction is actually more likely to churn

**Credit Limit vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "credit_limit", data = X_train);

"""The distribution of credit limit for churning user and not are relatively the same. We can drop this variable.

**Total Funds Used in One Period vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "funds used in period", data = X_train);

"""We can observe that the higher the total funds used in a period, the user will less likely to churn

**The difference between the credit limit and the current balance vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "avg_open_to_buy", data = X_train);

"""The distribution is pretty similar, we can remove this variable.

**Increase in customer transaction nominal between quarter 4 and quarter 1 vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "increase trans q4-q1", data = X_train);

"""The increase of transaction doesn't look significantly different between user who churn and not churn. We can drop this variable

**total nominal transaction vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "total_trans_amt", data = X_train);

"""The higher the total nominal transaction, the user will less likely to churn

**the number of transactions vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "total trans freq", data = X_train);

"""The higher number the transaction, the less likely the user will churn

**increased number of customer transactions between quarter 4 and quarter 1 vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "increase trans q4-q1", data = X_train);

"""The difference increased number of transaction between user who churn and not is slightly different. Let's try to keep this variable.

**Percentage of credit card usage vs attrition_flag**
"""

sns.boxplot(x = "attrition_flag", y = "avg cc usage", data = X_train);

"""The higher the percentage usage, the less like user will churn.

## Drop Columns

After observations, we'll drop some columns/variables that is not significant with the churn/attrition variable. Other than that, we need to drop the attrition flag too before modeling.
"""

X_train = X_train.drop(["gender","education_level","customer_age","dependent_count","months_on_book","credit_limit","avg_open_to_buy","increase trans q4-q1","total_trans_amt"], axis = 1)
X_test = X_test.drop(["gender","education_level","customer_age","dependent_count","months_on_book","credit_limit","avg_open_to_buy","increase trans q4-q1","total_trans_amt"], axis = 1)

X_train = X_train.drop(["attrition_flag"], axis = 1)

"""# Modeling

## Define Model

We will use 5 models:

- KNN as a baseline model
- Decision tree
- Random Forest
- SVM RBF
- XGBoost

**KNN**
"""

knn_clf = KNeighborsClassifier(
    n_neighbors = 5
)

"""**Decision Tree**"""

dc_clf = DecisionTreeClassifier(
    max_depth = 5,
    ccp_alpha = 0.001
)

"""**Random Forest**"""

rf_clf = RandomForestClassifier(
    random_state=1000,
    n_estimators=1000
)

"""**SVM RBF**"""

svm_clf = SVC(
    random_state = 1000,
    probability=True
)

"""**XGBoost**"""

xgb_clf = XGBClassifier(
    random_state=1000,
    n_estimators=1000
)

"""## Fitting Model to Data

**KNN**
"""

knn_clf.fit(X_train, y_train)

"""**Decision Tree**"""

dc_clf.fit(X_train, y_train)

"""**Random Forest**"""

rf_clf.fit(X_train, y_train)

"""**SVM RBF**"""

svm_clf.fit(X_train, y_train)

"""**XGBoost**"""

xgb_clf.fit(X_train, y_train)

"""## Model Evaluation"""

# knn prediction
knn_pred = knn_clf.predict(X_test)
knn_pred_proba = knn_clf.predict_proba(X_test)

# decision tree prediction
dc_pred = dc_clf.predict(X_test)
dc_pred_proba = dc_clf.predict_proba(X_test)

# random forest prediction
rf_pred = rf_clf.predict(X_test)
rf_pred_proba = rf_clf.predict_proba(X_test)

# SVM RBF prediction
svm_pred = svm_clf.predict(X_test)
svm_pred_proba = svm_clf.predict_proba(X_test)

# XGBoost prediction
xgb_pred = xgb_clf.predict(X_test)
xgb_pred_proba = xgb_clf.predict_proba(X_test)

"""### **KNN Evaluation**"""

pd.DataFrame(metrics.classification_report(y_test, knn_pred, target_names=['Not Churn','Churn'], output_dict=True))

skplt.metrics.plot_roc_curve(y_test, knn_pred_proba);

skplt.metrics.plot_confusion_matrix(y_test, knn_pred);

"""###**Decision Tree**"""

pd.DataFrame(metrics.classification_report(y_test, dc_pred, target_names=['Not Churn','Churn'], output_dict=True))

skplt.metrics.plot_roc_curve(y_test, dc_pred_proba);

# multi-layered perceptron result
skplt.metrics.plot_confusion_matrix(y_test, dc_pred);

"""###**Random Forest**"""

pd.DataFrame(metrics.classification_report(y_test, rf_pred, target_names=['Not Churn','Churn'], output_dict=True))

skplt.metrics.plot_roc_curve(y_test, rf_pred_proba);

skplt.metrics.plot_confusion_matrix(y_test, rf_pred);

"""###**SVM RBF**"""

pd.DataFrame(metrics.classification_report(y_test, svm_pred, target_names=['Not Churn','Churn'], output_dict=True))

skplt.metrics.plot_roc_curve(y_test, svm_pred_proba);

skplt.metrics.plot_confusion_matrix(y_test, svm_pred);

"""###**XGBoost**"""

pd.DataFrame(metrics.classification_report(y_test, xgb_pred, target_names=['Not Churn','Churn'], output_dict=True))

skplt.metrics.plot_roc_curve(y_test, xgb_pred_proba);

skplt.metrics.plot_confusion_matrix(y_test, xgb_pred);

"""After modelling, we found that **XGBoost** is the best model trained, shown by the f1-score for predict churn and not churn, accuracy, and the confusion matrix.

## Handle Imbalance Class Dataset with G-Means

### G-Means: KNN
"""

# find fpr and tpr using roc_curve() method
fpr, tpr, thresholds = metrics.roc_curve(y_test, knn_pred_proba[:,1])

# calculate g-means
knn_gmeans = np.sqrt(tpr * (1-fpr))

# find the best threshold
knn_ix = np.argmax(knn_gmeans)
print('Best Threshold={}, G-Mean={}'.format(thresholds[knn_ix], knn_gmeans[knn_ix]))

# change predict proba result to optimized one using new threshold
new_knn_pred = (knn_pred_proba[:,1] >= thresholds[knn_ix]).astype(int)

# compare the old f-1 score and new f1-score
display("old f1-score = ",metrics.f1_score(y_test, knn_pred))
display("new f1-score = ", metrics.f1_score(y_test, new_knn_pred))

"""### G-Means: Decision Tree"""

# find fpr and tpr using roc_curve() method
fpr, tpr, thresholds = metrics.roc_curve(y_test, dc_pred_proba[:,1])

# calculate g-means
dc_gmeans = np.sqrt(tpr * (1-fpr))

# find the best threshold
dc_ix = np.argmax(dc_gmeans)
print('Best Threshold={}, G-Mean={}'.format(thresholds[dc_ix], dc_gmeans[dc_ix]))

# change predict proba result to optimized one using new threshold
new_dc_pred = (dc_pred_proba[:,1] >= thresholds[dc_ix]).astype(int)

# compare the old f-1 score and new f1-score
display("old f1-score = ",metrics.f1_score(y_test, dc_pred))
display("new f1-score = ", metrics.f1_score(y_test, new_dc_pred))

"""### G-Means: Random Forest"""

# find fpr and tpr using roc_curve() method
fpr, tpr, thresholds = metrics.roc_curve(y_test, rf_pred_proba[:,1])

# calculate g-means
rf_gmeans = np.sqrt(tpr * (1-fpr))

# find the best threshold
rf_ix = np.argmax(rf_gmeans)
print('Best Threshold={}, G-Mean={}'.format(thresholds[rf_ix], rf_gmeans[rf_ix]))

# change predict proba result to optimized one using new threshold
new_rf_pred = (rf_pred_proba[:,1] >= thresholds[rf_ix]).astype(int)

# compare the old f-1 score and new f1-score
display("old f1-score = ",metrics.f1_score(y_test, rf_pred))
display("new f1-score = ", metrics.f1_score(y_test, new_rf_pred))

"""### G-Means: SVM"""

# find fpr and tpr using roc_curve() method
fpr, tpr, thresholds = metrics.roc_curve(y_test, svm_pred_proba[:,1])

# calculate g-means
svm_gmeans = np.sqrt(tpr * (1-fpr))

# find the best threshold
svm_ix = np.argmax(svm_gmeans)
print('Best Threshold={}, G-Mean={}'.format(thresholds[svm_ix], svm_gmeans[svm_ix]))

# change predict proba result to optimized one using new threshold
new_svm_pred = (svm_pred_proba[:,1] >= thresholds[svm_ix]).astype(int)

# compare the old f-1 score and new f1-score
display("old f1-score = ",metrics.f1_score(y_test, svm_pred))
display("new f1-score = ", metrics.f1_score(y_test, new_svm_pred))

"""### G-Means: XGBoost"""

# find fpr and tpr using roc_curve() method
fpr, tpr, thresholds = metrics.roc_curve(y_test, xgb_pred_proba[:,1])
# calculate g-means
xgb_gmeans = np.sqrt(tpr * (1-fpr))

# find the best threshold
xgb_ix = np.argmax(xgb_gmeans)
print('Best Threshold={}, G-Mean={}'.format(thresholds[xgb_ix], xgb_gmeans[xgb_ix]))

# change predict proba result to optimized one using new threshold
new_xgb_pred = (xgb_pred_proba[:,1] >= thresholds[xgb_ix]).astype(int)

# compare the old f-1 score and new f1-score
display("old f1-score = ",metrics.f1_score(y_test, xgb_pred))
display("new f1-score = ", metrics.f1_score(y_test, new_xgb_pred))

old_knn = metrics.f1_score(y_test, knn_pred)
new_knn = metrics.f1_score(y_test, new_knn_pred)
old_dc = metrics.f1_score(y_test, dc_pred)
new_dc = metrics.f1_score(y_test, new_dc_pred)
old_rf = metrics.f1_score(y_test, rf_pred)
new_rf = metrics.f1_score(y_test, new_rf_pred)
old_svm = metrics.f1_score(y_test, svm_pred)
new_svm = metrics.f1_score(y_test, new_svm_pred)
old_xgb = metrics.f1_score(y_test, xgb_pred)
new_xgb = metrics.f1_score(y_test, new_xgb_pred)

display("old KNN f1-score = ", old_knn)
display("new KNN f1-score = ", new_knn)
display("old DC f1-score = ", old_dc)
display("new DC f1-score = ", new_dc)
display("old RF f1-score = ", old_rf)
display("new RF f1-score = ", new_rf)
display("old SVM f1-score = ", old_svm)
display("new SVM f1-score = ", new_svm)
display("old XGB f1-score = ", old_xgb)
display("new SVM f1-score = ", new_xgb)

"""# Explanatory Model Analysis"""

## initiate explainer for Random Forest model
churn_rf_exp = dx.Explainer(rf_clf, X_train, y_train, label = "RF Interpretation")

## initiate explainer for XGBoost model
churn_xgb_exp = dx.Explainer(xgb_clf, X_train, y_train, label = "XGBoost Interpretation")

"""## Feature Importance"""

# visualize permutation feature importance for Random Forest Model
churn_rf_exp.model_parts().plot()

# visualize permutation feature importance for XGBoost model
churn_xgb_exp.model_parts().plot()

"""## Partial Dependence Plot"""

# create partial dependence plot of Random Forest model
churn_rf_exp.model_profile().plot()

# create partial dependence plot of XGBoost model
churn_xgb_exp.model_profile().plot()